# -*- coding: utf-8 -*-
"""ML_HW4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11bTZp4-q5CFtHgHolg1jpHHs9Wc8_7uW
"""

# Libraries

import os
from time import time
from tqdm import tqdm


import torch
from torch.nn import Linear, CrossEntropyLoss
from torch.optim import Adam
from torch.utils.data import DataLoader

from torchvision.datasets import ImageFolder
from torchvision.models import resnet18
from torchvision.transforms import transforms


import numpy as np
import glob
from collections import namedtuple
import os
import cv2
import math
import shutil
import time

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

classes = ["squirrel", "spyder", "sheep", "horse", "elephant", "dog", "cow", "chicken", "cat", "butterfly"]

#read data and find the largest and smallest size of all images
max_hei = 0
max_wid = 0
min_hei = 1000
min_wid = 1000

for Pic in glob.glob('training/*'):
    Label = Pic.replace('training/', '')
    count = 0   #count the number of data in each class
    for pic in glob.glob(Pic + '/*.jpeg'):
        count += 1
        pic = cv2.imread(pic)
        if pic.shape[0] > max_hei:
            max_hei = pic.shape[0]
        if pic.shape[1] > max_wid:
            max_wid = pic.shape[1]
        if pic.shape[0] < min_hei:
            min_hei = pic.shape[0]
        if pic.shape[1] < min_wid:
            min_wid = pic.shape[1]
    print(Label, count)

print("max shape:", max_hei, max_wid)
print("min shape:", min_hei, min_wid)
temp_hei = (max_hei + min_hei) / 2
temp_wid = (max_wid + min_wid) / 2
image_hei = pow(2, math.ceil(math.log(temp_hei)/math.log(2)))
image_wid = pow(2, math.ceil(math.log(temp_wid)/math.log(2)))

print(f"final width, height{image_wid, image_hei}")
# Data Transformer
tfm = transforms.Compose([
    transforms.Resize((image_hei, image_wid)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(degrees=10),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

'''TRAIN_RATIO = 0.8

ROOT = ''
images_dir = os.path.join(ROOT, 'training')
train_dir = os.path.join(ROOT, 'train')
test_dir = os.path.join(ROOT, 'test')

#if the path already exists, remove it
if os.path.exists(train_dir):
    shutil.rmtree(train_dir) 
if os.path.exists(test_dir):
    shutil.rmtree(test_dir)

#create new folders with the path names 'train' and 'test    
os.makedirs(train_dir)
os.makedirs(test_dir)

#list all folders under the path, which gives us every categories(label) of data
classes = os.listdir(images_dir)

for c in classes:
    #path to specific folder
    class_dir = os.path.join(images_dir, c)
    #list all images in folder
    images = os.listdir(class_dir)
       
    n_train = int(len(images) * TRAIN_RATIO)

    #split training and testing data
    train_images = images[:n_train]
    test_images = images[n_train:]

    #create folders under 'train' and 'test' folders
    os.makedirs(os.path.join(train_dir, c), exist_ok = True)
    os.makedirs(os.path.join(test_dir, c), exist_ok = True)

    #iterate over images
    for image in train_images:
        image_src = os.path.join(class_dir, image)
        image_dst = os.path.join(train_dir, c, image) 
        #copy image from 'training' folder to train folder
        shutil.copyfile(image_src, image_dst)
        
    for image in test_images:
        image_src = os.path.join(class_dir, image)
        image_dst = os.path.join(test_dir, c, image) 
        #copy image from 'training folder to test folder'
        shutil.copyfile(image_src, image_dst)'''

# Create Dataset
trainRoot = "train"
testRoot = "test"

train_ds = ImageFolder(trainRoot, transform = tfm)
test_ds = ImageFolder(testRoot, transform = tfm)

# Length of Train and Test Datasets
len_train = len(train_ds)
len_test = len(test_ds)
print(len_train, len_test)

# map class to index (butterfly -> 0, cat -> 1, ...)
print(train_ds.class_to_idx)

# Data Loader
train_loader = DataLoader(train_ds, batch_size = 30, shuffle = True)
test_loader = DataLoader(test_ds, batch_size = 30, shuffle = True)

# Model - use pretrained resnet
model = resnet18(pretrained=True)

# Replace Output of Fully Connected Layer with #labels(regard of cases given)
model.fc = Linear(in_features=512, out_features=10)
#train on GPU
model = model.to(device)
# model = model.cuda()    # only if your system supports Nvidia CUDA
print(model)

#optimizer
optimizer = Adam(model.parameters(), lr=3e-4, weight_decay=0.0001)

# Loss Function
loss_fn = CrossEntropyLoss()

from time import time
count = 0
for epoch in range(3):
    start = time()
    
    tr_acc = 0
    test_acc = 0
    
    # Train
    model.train()
    
    for xtrain, ytrain in tqdm(train_loader, unit="batch"):
        
        optimizer.zero_grad()
        #use GPU for training  
        xtrain = xtrain.to(device)
        train_prob = model(xtrain)
        train_prob = train_prob.cpu()
        
        loss = loss_fn(train_prob, ytrain)
        loss.backward()
        optimizer.step()
        
        # training ends
        
        train_pred = torch.max(train_prob, 1).indices
        tr_acc += int(torch.sum(train_pred == ytrain))
            
    ep_tr_acc = tr_acc / len_train
    
    # Evaluate
    model.eval()
    with torch.no_grad():
        for xtest, ytest in test_loader:
            xtest = xtest.to(device)
            test_prob = model(xtest)
            test_prob = test_prob.cpu()
            
            test_pred = torch.max(test_prob,1).indices
            test_acc += int(torch.sum(test_pred == ytest))
            
        ep_test_acc = test_acc / len_test
    
    end = time()
    duration = (end - start) / 60
    
    print(f"Epoch: {epoch}, Time: {duration}, Loss: {loss}\nTrain_acc: {ep_tr_acc}, Test_acc: {ep_test_acc}")

